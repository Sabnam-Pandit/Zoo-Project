{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dd8c346-655f-43bf-b5c1-507dea31e48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.11/site-packages (3.11.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from h5py) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.14.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: umap-learn in /opt/conda/lib/python3.11/site-packages (0.5.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (1.5.1)\n",
      "Requirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (0.60.0)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.11/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from umap-learn) (4.66.4)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.11/site-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.11/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
      "Requirement already satisfied: plotly in /opt/conda/lib/python3.11/site-packages (6.0.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /opt/conda/lib/python3.11/site-packages (from plotly) (1.37.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from plotly) (24.1)\n",
      "Requirement already satisfied: hdbscan in /opt/conda/lib/python3.11/site-packages (0.8.40)\n",
      "Requirement already satisfied: numpy<3,>=1.20 in /opt/conda/lib/python3.11/site-packages (from hdbscan) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.11/site-packages (from hdbscan) (1.14.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/conda/lib/python3.11/site-packages (from hdbscan) (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.11/site-packages (from hdbscan) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn>=0.20->hdbscan) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing the packages\n",
    "!pip install h5py scikit-learn\n",
    "!pip install umap-learn\n",
    "!pip install plotly\n",
    "!pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0737649-f091-4932-878e-60c802f325d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.20.1%2Bcu121-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
      "Collecting torchaudio\n",
      "  Using cached https://download.pytorch.org/whl/cu121/torchaudio-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_nvjitlink_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (19.8 MB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.13.0\n",
      "    Uninstalling sympy-1.13.0:\n",
      "      Successfully uninstalled sympy-1.13.0\n",
      "Successfully installed filelock-3.13.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.1.105 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 torchaudio-2.5.1+cu121 torchvision-0.20.1+cu121 triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd429d8-5229-433e-808c-ff2b564152dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "import joblib\n",
    "import hdbscan\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.decomposition import PCA\n",
    "#from sklearn.cluster import DBSCAN\n",
    "import umap.umap_ as umap\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "import plotly.io as pio   \n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "\n",
    "import umap\n",
    "#from sklearn.cluster import DBSCAN\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "894c270b-baa0-4d30-997a-1f957b1a5918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (71.0.4)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Using cached optree-0.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
      "Using cached absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "Using cached keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (410 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, optree, opt-einsum, ml-dtypes, mdurl, markdown, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.2.2 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 keras-3.9.2 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.0.9 opt-einsum-3.4.0 optree-0.15.0 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 werkzeug-3.1.3 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a8d29e-87a5-4a9b-9df0-0369287d0dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaleido\n",
      "  Using cached kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Using cached kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
      "Installing collected packages: kaleido\n",
      "Successfully installed kaleido-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fb2093",
   "metadata": {},
   "source": [
    "### Performing HDBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b18384-8f63-430e-bc16-5ff844de5422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 20:46:42.008791: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-03 20:46:42.023739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746305202.040292     465 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746305202.045313     465 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746305202.059046     465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746305202.059063     465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746305202.059064     465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746305202.059066     465 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-03 20:46:42.063933: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Loading embeddings...\n",
      "Loaded embeddings in 4.81 seconds.\n",
      "Total embeddings: 2293146\n",
      "\n",
      "Scaling embeddings...\n",
      "Scaled embeddings in 22.74 seconds.\n",
      "\n",
      "Reducing dimensions with UMAP...\n",
      "UMAP reduction completed in 2845.06 seconds.\n",
      "\n",
      "Applying HDBSCAN clustering...\n",
      "HDBSCAN clustering completed in 867.68 seconds.\n",
      "HDBSCAN produced 101 clusters (including noise).\n",
      "  Number of noise points (-1): 218660\n",
      "\n",
      "Saving clustering results before evaluation...\n",
      "Saved to 'pre_evaluation_outputs.npz'\n",
      "\n",
      "Mapping cluster-to-file assignments and computing centroids...\n",
      "Cluster mapping and centroids saved.\n",
      "  Noise Proportion: 9.54%\n",
      "\n",
      "Full clustering process completed in 3769.56 seconds!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import hdbscan\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from scipy.spatial.distance import cdist\n",
    "import umap\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# Load the embeddings from the h5 files\n",
    "def get_embeddings_batch(batch_files, folder_path='Embeddings'):\n",
    "    all_embeddings = []\n",
    "    all_file_names_with_indices = []\n",
    "    for file_name in batch_files:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            data_key = list(f.keys())[0]\n",
    "            embeddings = f[data_key][:]\n",
    "            embeddings = torch.tensor(embeddings, device=device)\n",
    "            all_embeddings.append(embeddings)\n",
    "\n",
    "            parts = file_name.split('_')\n",
    "            base_name = f\"{parts[0]}_{parts[1]}\"\n",
    "            num_embeddings = embeddings.shape[0]\n",
    "            all_file_names_with_indices.extend([f\"{base_name}_{i}\" for i in range(num_embeddings)])\n",
    "        \n",
    "        #print(f\"Loaded {file_name} with {num_embeddings} embeddings\")\n",
    "\n",
    "    combined_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    return combined_embeddings, all_file_names_with_indices\n",
    "\n",
    "def apply_clustering(total_files, folder_path='Embeddings'):\n",
    "    overall_start = time.time()\n",
    "\n",
    "    print(\"\\nLoading embeddings...\")\n",
    "    load_start = time.time()\n",
    "    h5_files = [file for file in os.listdir(folder_path) if file.endswith('.h5')][:total_files]\n",
    "    embeddings, file_names = get_embeddings_batch(h5_files, folder_path)\n",
    "    load_end = time.time()\n",
    "    print(f\"Loaded embeddings in {load_end - load_start:.2f} seconds.\")\n",
    "    print(f\"Total embeddings: {embeddings.shape[0]}\")\n",
    "\n",
    "    print(\"\\nScaling embeddings...\")\n",
    "    scale_start = time.time()\n",
    "    embeddings_np = embeddings.cpu().numpy()\n",
    "# Scaling the embeddings\n",
    "    global_scaler = StandardScaler()\n",
    "    scaled_embeddings_np = global_scaler.fit_transform(embeddings_np)\n",
    "    scale_end = time.time()\n",
    "    print(f\"Scaled embeddings in {scale_end - scale_start:.2f} seconds.\")\n",
    "# Reducing dimensions with UMAP: 758 to 50 dimensions\n",
    "    print(\"\\nReducing dimensions with UMAP...\")\n",
    "    umap_start = time.time()\n",
    "    umap_model = umap.UMAP(n_components=50, n_neighbors=50, min_dist=0.05, metric='cosine')\n",
    "    reduced_embeddings_np = umap_model.fit_transform(scaled_embeddings_np)\n",
    "    umap_end = time.time()\n",
    "    print(f\"UMAP reduction completed in {umap_end - umap_start:.2f} seconds.\")\n",
    "# Applying HDBSCAN clustering \n",
    "    print(\"\\nApplying HDBSCAN clustering...\")\n",
    "    hdbscan_start = time.time()\n",
    "    clusterer = hdbscan.HDBSCAN(min_samples=30, min_cluster_size=100, cluster_selection_epsilon=0.1)\n",
    "    labels = clusterer.fit_predict(reduced_embeddings_np)\n",
    "    hdbscan_end = time.time()\n",
    "    print(f\"HDBSCAN clustering completed in {hdbscan_end - hdbscan_start:.2f} seconds.\")\n",
    "    print(f\"HDBSCAN produced {len(set(labels))} clusters (including noise).\")\n",
    "    print(f\"  Number of noise points (-1): {np.sum(labels == -1)}\")\n",
    "\n",
    "    # Save pre-evaluation results\n",
    "    print(\"\\nSaving clustering results before evaluation...\")\n",
    "    np.savez_compressed(\"full_evaluation_outputs.npz\",\n",
    "                        embeddings=reduced_embeddings_np,\n",
    "                        labels=labels,\n",
    "                        unique_ids=np.array(file_names))\n",
    "    print(\"Saved to 'pre_evaluation_outputs.npz'\")\n",
    "\n",
    "    # Map files to clusters and compute centroids\n",
    "    print(\"\\nMapping cluster-to-file assignments and computing centroids...\")\n",
    "    cluster_to_files = {}\n",
    "    cluster_centroids = {}\n",
    "\n",
    "    for label in set(labels):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        cluster_indices = np.where(labels == label)[0]\n",
    "        cluster_embeddings = reduced_embeddings_np[cluster_indices]\n",
    "        cluster_ids = [file_names[i] for i in cluster_indices]\n",
    "        cluster_to_files[label] = cluster_ids\n",
    "\n",
    "        centroid = np.mean(cluster_embeddings, axis=0)\n",
    "        distances = cdist([centroid], cluster_embeddings, 'euclidean')\n",
    "        closest_index = np.argmin(distances)\n",
    "        closest_id = cluster_ids[closest_index]\n",
    "        cluster_centroids[label] = closest_id\n",
    "\n",
    "    with open(\"cluster_to_files_full.json\", \"w\") as f:\n",
    "        json.dump({int(k): v for k, v in cluster_to_files.items()}, f, indent=4)\n",
    "\n",
    "    with open(\"cluster_centroids_full.json\", \"w\") as f:\n",
    "        json.dump({int(k): v for k, v in cluster_centroids.items()}, f, indent=4)\n",
    "\n",
    "    print(\"Cluster mapping and centroids saved.\")\n",
    "\n",
    "    # Checking Number of Resulting Noise Points. \n",
    "    noise_ratio = np.sum(labels == -1) / len(labels)\n",
    "    print(f\"  Noise Proportion: {noise_ratio:.2%}\")\n",
    "\n",
    "    gc.collect()\n",
    "    overall_end = time.time()\n",
    "    print(f\"\\nFull clustering process completed in {overall_end - overall_start:.2f} seconds!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    total_files = 192 \n",
    "    apply_clustering(total_files=total_files, folder_path='../Embeddings')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad910d1",
   "metadata": {},
   "source": [
    "### Evaluation of the Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09ddc7",
   "metadata": {},
   "source": [
    "We use silhouette_score and calinski_harabasz_score to evaluate quality of the resulting clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4dfca-a53b-454f-92b2-dddcc1a8a65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading clustering results from: pre_evaluation_outputs.npz\n",
      "Loaded 960480 embeddings with 75 unique clusters (including noise).\n",
      "  Number of noise points (-1): 93475\n",
      "  Noise Proportion: 9.73%\n",
      "\n",
      "Evaluating clustering quality...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "def evaluate_clustering(file_path=\"pre_evaluation_outputs.npz\"):\n",
    "    print(f\"Loading clustering results from: {file_path}\")\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "\n",
    "    embeddings = data['embeddings']\n",
    "    labels = data['labels']\n",
    "    file_names = data['unique_ids']\n",
    "\n",
    "    print(f\"Loaded {len(labels)} embeddings with {len(set(labels))} unique clusters (including noise).\")\n",
    "    print(f\"  Number of noise points (-1): {np.sum(labels == -1)}\")\n",
    "    print(f\"  Noise Proportion: {np.sum(labels == -1) / len(labels):.2%}\")\n",
    "\n",
    "    print(\"\\nEvaluating clustering quality...\")\n",
    "    try:\n",
    "        silhouette = silhouette_score(embeddings, labels)\n",
    "        print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Silhouette Score could not be calculated: {e}\")\n",
    "\n",
    "    try:\n",
    "        ch_index = calinski_harabasz_score(embeddings, labels)\n",
    "        print(f\"  Calinski-Harabasz Index: {ch_index:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Calinski-Harabasz Index could not be calculated: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_clustering(\"pre_evaluation_outputs.npz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736184d4",
   "metadata": {},
   "source": [
    "### Plotly for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "820ffccc-c9bd-46db-9992-dbdac3ec8e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kaleido\n",
      "  Using cached kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl.metadata (15 kB)\n",
      "Using cached kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
      "Installing collected packages: kaleido\n",
      "Successfully installed kaleido-0.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install kaleido"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ce7e77",
   "metadata": {},
   "source": [
    "We are plotting resulting clusters using Plotly. In order to be able to display them, initially dimensionality has to be reduced from 50D to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd8802d-056a-4843-9236-593e3b51c527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaleido\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "import umap\n",
    "\n",
    "# Setting renderer for interactive plot\n",
    "pio.renderers.default = 'notebook'  # or 'browser' if you want it to open in a browser\n",
    "\n",
    "# Loading saved outputs\n",
    "outputs = np.load(\"full_evaluation_outputs.npz\") #input your file name here\n",
    "reduced_embeddings_highd = outputs['embeddings']  # These are still 50D\n",
    "labels = outputs['labels']\n",
    "\n",
    "# Perform UMAP reduction to 3D for visualization\n",
    "print(\"Reducing embeddings to 3D for visualization...\")\n",
    "umap_3d = umap.UMAP(n_components=3, n_neighbors=40, min_dist=0.5, metric='euclidean')\n",
    "reduced_embeddings_3d = umap_3d.fit_transform(reduced_embeddings_highd)\n",
    "\n",
    "# Create a color palette for the clusters\n",
    "unique_labels = np.unique(labels)\n",
    "colors = px.colors.qualitative.Plotly  # Default Plotly color scheme\n",
    "color_map = {label: colors[i % len(colors)] for i, label in enumerate(unique_labels)}\n",
    "\n",
    "# Build the 3D scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "for label in unique_labels:\n",
    "    mask = labels == label\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=reduced_embeddings_3d[mask, 0],\n",
    "            y=reduced_embeddings_3d[mask, 1],\n",
    "            z=reduced_embeddings_3d[mask, 2],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                color=color_map[label],\n",
    "                opacity=0.8\n",
    "            ),\n",
    "            name=f\"Cluster {label}\" if label != -1 else \"Noise\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title=\"Interactive 3D Plot of HDBSCAN Clustering (UMAP 3D Reduced)\",\n",
    "    scene=dict(\n",
    "        xaxis_title='UMAP Dimension 1',\n",
    "        yaxis_title='UMAP Dimension 2',\n",
    "        zaxis_title='UMAP Dimension 3',\n",
    "    ),\n",
    "    legend_title=\"Clusters\",\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
